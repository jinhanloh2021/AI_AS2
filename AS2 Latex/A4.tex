\item You can build your solution on top of the python notebook covered in class to classify the
standard MNIST dataset.\\[5pt]
Full solution found in \lstinline{Q4_template_MNISTCorrupted.ipynb}.
\begin{lstlisting}
  from matplotlib import pyplot as plt
  import numpy as np
  import tensorflow as tf
  from tensorflow.keras import layers
  import tensorflow_datasets as tfds

  ## write your code here
  dataset_name = "mnist_corrupted/zigzag"
  train_ds = tfds.load(dataset_name, split='train', batch_size=-1, as_supervised=True)
  test_ds = tfds.load(dataset_name, split='test', batch_size=-1, as_supervised=True)

  train_images, train_labels = tfds.as_numpy(train_ds)
  test_images, test_labels   = tfds.as_numpy(test_ds)

  # Test size of different loaded numpy arrays
  print('Image size:', train_images[0].shape)
  print('Training data size:',train_images.shape)
  print('Training labels size:', train_labels.shape)
  print('Testing data size:', test_images.shape)

  model = tf.keras.Sequential()
  outputs = 10 #because there are 10 digits in mnist
  ## write your code here to build your dense ANN. Input layer is created below
  model.add(layers.Flatten(input_shape=(train_images[0].shape)))
  model.add(layers.Dense(10, activation=tf.nn.relu))
  model.add(layers.Dense(20, activation=tf.nn.relu))
  model.add(layers.Dense(20, activation=tf.nn.relu))
  model.add(layers.Dense(60, activation=tf.nn.relu))
  model.add(layers.Dense(60, activation=tf.nn.relu))
  model.add(layers.Dense(80, activation=tf.nn.relu))
  model.add(layers.Dense(80, activation=tf.nn.relu))
  model.add(layers.Dense(100, activation=tf.nn.relu))
  model.add(layers.Dense(100, activation=tf.nn.relu))
  model.add(layers.Dense(80, activation=tf.nn.relu))
  model.add(layers.Dense(80, activation=tf.nn.relu))
  model.add(layers.Dense(60, activation=tf.nn.relu))
  model.add(layers.Dense(60, activation=tf.nn.relu))
  model.add(layers.Dense(20, activation=tf.nn.relu))
  model.add(layers.Dense(20, activation=tf.nn.relu))
  model.add(layers.Dense(10, activation=tf.nn.softmax))
  model.summary()

  ### write your code here to compile model
  model.compile(optimizer="Adam", loss='sparse_categorical_crossentropy', metrics=['accuracy'])

  ### write your code here to train your model
  epochs = 10
  history = model.fit(train_images, train_labels, epochs=epochs)

  plt.plot(history.history["loss"])

  #### write your code to report overall accuracy on test set
  test_results = model.evaluate(test_images, test_labels, return_dict=True)
  # print(test_results)
  print('Test accuracy:', test_results['accuracy'])

  ### write your code to report per-class accuracy
  ### Use confusion matrix from sklearn. 
  from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
  image_pred = model.predict(test_images)
  image_pred_classes = image_pred.argmax(axis=-1)
  labels = [0,1,2,3,4,5,6,7,8,9]
  cm = confusion_matrix(test_labels, image_pred_classes, labels=labels)
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)
  disp.plot()

  import matplotlib.pyplot as plt
  %matplotlib inline
  class_names = ['0','1','2','3','4','5','6','7','8','9']
  plt.figure(figsize=(10,10))
  for i in range(25):
      plt.subplot(5,5,i+1)
      plt.xticks([])
      plt.yticks([])
      plt.grid(False)
      plt.imshow(train_images[i].reshape(28, 28), cmap=plt.cm.binary)
      #print(train_labels[i][0])
      plt.xlabel(class_names[train_labels[i]])
\end{lstlisting}
\clearpage