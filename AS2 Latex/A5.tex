\item In this question, we will learn how to use transfer learning in the context of CNNs.

\begin{enumerate}
  \item Create the LeNet-5 CNN architecture using Keras API (see code skeleton for the number
        and types of layers to create). Train the model on the MNIST dataset. {\bf [3 points]}
        \begin{lstlisting}
          input_shape = train_images[0].shape
          model = tf.keras.Sequential()
          model.add(layers.Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='same', input_shape=input_shape))
          model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
          model.add(layers.Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))
          model.add(tf.keras.layers.AveragePooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None))
          model.add(layers.Conv2D(120, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))
          model.add(layers.Flatten())
          model.add(layers.Dense(84, activation='tanh'))
          model.add(layers.Dense(10, activation='softmax'))
          model.summary()

          # Compile the model with appropriate Loss function
          model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), 
                        loss='sparse_categorical_crossentropy',
                        metrics=['accuracy'])

          # Train the model on MNIST dataset
          epochs = 5
          batch_size = 512
          model.fit(train_images, train_labels, batch_size=batch_size, epochs=epochs)

          test_loss, test_acc = model.evaluate(test_images, test_labels)
          print('Test accuracy:', test_acc) #0.97
        \end{lstlisting}
  \item What is the accuracy of your trained LeNet-5 model on the MNIST training dataset? Try
        to get an accuracy above 90\%. {\bf [0.5 points]}
        $$\text{Test Accuracy} = 0.9735$$
  \item Download the \lstinline{binary_alpha_digits} dataset using tfds, and split the dataset into 20\%
        testing data and 80\% training data.{\bf [1 point]}
        \begin{lstlisting}
          ## write your code here
          dataset_name = "binary_alpha_digits"
          train_ds, test_ds = tfds.load(dataset_name, split=['train[0\%:80\%]','train[80\%:100\%]'], batch_size=-1, as_supervised=True)

          train_images, train_labels = tfds.as_numpy(train_ds)
          test_images, test_labels   = tfds.as_numpy(test_ds)

          print('Image size:', train_images[0].shape)
          print('Training data size:',train_images.shape)
          print('Training labels size:', train_labels.shape)
          print('Testing data size:', test_images.shape)
        \end{lstlisting}
  \item As the dimension of images in the \lstinline{binary_alpha_digits} are different from the image size in MNIST dataset, upscale images in \lstinline{binary_alpha_digits} to match the image size in MNIST dataset using OpenCV. This is required as we would like to use the LeNet trained using the MNIST dataset for \lstinline{binary_alpha_digits}. {\bf [2 points]}
        \begin{lstlisting}
          newSize = 28
          # create a numpy array for storing upscaled training images
          train_upscale = np.zeros((train_images.shape[0], newSize, newSize, 1))
          for i, t in enumerate(train_images):
            resized = cv2.resize(t, (newSize, newSize))
            train_upscale[i] = resized.reshape(newSize, newSize, 1)
          print("Train upscale shape: ",train_upscale.shape)
          print("Train images shape: ", train_images.shape)

          # create a numpy array for storing upscaled testing images
          test_upscale = np.zeros((test_images.shape[0], newSize, newSize, 1))
          for i, t in enumerate(test_images):
            resized = cv2.resize(t, (newSize, newSize))
            test_upscale[i] = resized.reshape(newSize, newSize, 1)

          print("Test upscale shape:  ", test_upscale.shape)
          print("Test images shape: ", test_images.shape)
        \end{lstlisting}
  \item Remove the final output layer of LeNet you have trained on MNIST (to do this, please check the flag "include top" in Keras and the tensorflow link for transfer learning noted earlier) {\bf [0.5 points]}
        \begin{lstlisting}
          transfer_model = tf.keras.Sequential()
          for layer in model.layers[:-3]: #remove last three layers
            transfer_model.add(layer)

          #Freeze layers
          transfer_model.get_layer('conv2d').trainable = False

          print(f"All layers: {list(layer.name for layer in transfer_model.layers)}.")
          print(f"Frozen layers: {list(layer.name for layer in transfer_model.layers if (not transfer_model.get_layer(layer.name).trainable))}.")
        \end{lstlisting}
  \item After removing the final output layer, extend your trained LeNet model by adding at least one hidden layer (dense, convolution, max pooling or any other type of layer). Also attach one final output layer. In this part, you are free to explore and decide how many hidden layers to add, their type, the number of nodes in each layer and the activation function yourself. Keep in mind, the output layer must have the appropriate number of nodes and activation function that matches the given task. {\bf [1.5 points]}
        \begin{lstlisting}
          transfer_model.add(layers.Flatten())
          transfer_model.add(layers.Dense(84, activation='tanh'))
          transfer_model.add(layers.Dense(64, activation='tanh'))
          transfer_model.add(layers.Dense(36, activation='softmax')) #36 classes
          transfer_model.summary()
        \end{lstlisting}
  \item Train the model and show accuracy on the testing dataset (of \lstinline{binary_alpha_digits}). You can either fix all the weights of your MNIST-trained LeNet model and train only the layers you have added, or train the whole network again. Choose the setting that gives you higher accuracy given the computational resources. Check link https://keras.io/getting-started/faq/\#how-can-i-freeze-keras-layers. Try to achieve a testing data accuracy of 50\% or more (you can report the best over multiple runs). Please make sure that in your submitted jupypter notebook, logs show your best run. Note: some variation between runs is expected, with the true accuracy being somewhere in between. You are not required to reliably get 50 percent accuracy over all runs, but try to demonstrate from the log files that one run achieved 50 percent. {\bf [1.5 points]}
        \begin{lstlisting}
          transfer_model.compile(optimizer=tf.optimizers.Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
          
          epochs = 40
          batch_size = 512
          transfer_model.fit(train_upscale, train_labels, batch_size=batch_size, epochs=epochs)

          test_loss, test_acc = transfer_model.evaluate(test_upscale, test_labels)
          print('Test accuracy:', test_acc)
        \end{lstlisting}
        $$\text{Test Accuracy}=0.7224$$
\end{enumerate}
