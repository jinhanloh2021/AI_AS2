{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjHK80jR15cb",
        "outputId": "478e5f25-14b9-46a8-c3d1-b2d785fe8ed5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import word2vec\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import re # For regular expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTam_xXUv2Z9"
      },
      "source": [
        "## (a) Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQsTJcH2X-9E",
        "outputId": "4d9fa42b-4602-423a-84b1-a00caf307f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['trending', 'new', 'yorkers', 'encounter', 'empty', 'supermarket', 'shelves', 'pictured', 'wegmans', 'brooklyn', 'sold', 'online', 'grocers', 'foodkick', 'maxdelivery', 'coronavirus', 'fearing', 'shoppers', 'stock'], ['when', 'i', 'find', 'hand', 'sanitizer', 'fred', 'meyer', 'i', 'turned', 'amazon', 'but', 'pack', 'purell', 'check', 'coronavirus', 'concerns', 'driving', 'prices'], ['find', 'protect', 'loved', 'ones', 'coronavirus']]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "def load_data():\n",
        "    \"\"\" Read tweets from the file.\n",
        "        Return:\n",
        "            list of lists (list_words), with words from each of the processed tweets\n",
        "    \"\"\"\n",
        "    tweets = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI_AS2/Corona_Tweets.csv', names=['text'])\n",
        "    list_words = []\n",
        "    ### iterate over all tweets from the dataset\n",
        "    for i in tweets.index:\n",
        "      ### remove URLs\n",
        "      text = re.sub(\"https?://\\S+|www\\.\\S+\", \" \", tweets.loc[i, 'text'])\n",
        "      ### remove non-letter.\n",
        "      text = re.sub(\"[^a-zA-Z]\",\" \",text)\n",
        "      ### tokenize\n",
        "      words = text.split()\n",
        "      \n",
        "      new_words = []\n",
        "      ### iterate over all words of a tweet\n",
        "      for w in words:\n",
        "        ## TODO: remove the stop words and convert a word (w) to the lower case\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        if w not in stops:\n",
        "          new_words.append(w.lower())\n",
        "        \n",
        "      list_words.append(new_words)\n",
        "    return list_words\n",
        "\n",
        "# check a few samples of twitter corpus\n",
        "twitter_corpus = load_data()\n",
        "print(twitter_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ZkbmSX15ck"
      },
      "source": [
        "## (b) Create co-occurrence matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3B83uir15cn",
        "outputId": "b261769c-ed9c-46c4-dcae-3ca1dbe64b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['a', 'aadya', 'aadyasitara', 'aamiin', 'aapl', 'abajam', 'abandon', 'abandoning', 'abc', 'abeg'] 11454\n"
          ]
        }
      ],
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" get a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "        Return:\n",
        "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = set()\n",
        "    for tweet in corpus:\n",
        "      for word in tweet:\n",
        "        corpus_words.add(word)\n",
        "    corpus_words = sorted(list(corpus_words))\n",
        "    num_corpus_words = len(corpus_words)\n",
        "    return corpus_words, num_corpus_words\n",
        "\n",
        "words, num_words = distinct_words(twitter_corpus)\n",
        "print(words[:10], num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8WB4cZBR15cp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=5):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 5).    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (numpy matrix of shape = [number of corpus words x number of corpus words]): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    M = np.zeros((num_words, num_words), dtype=int)\n",
        "    word2Ind = {}\n",
        "    for i, w in enumerate(words):\n",
        "      word2Ind[w] = i\n",
        "    for tweet in corpus:\n",
        "      for i, w in enumerate(tweet):\n",
        "        w_idx = word2Ind[w]\n",
        "        start = i - 5\n",
        "        end = i + 5 + 1 #exclusive\n",
        "        for j in range(start, end):\n",
        "          if(i != j and j >= 0 and j < len(tweet)):\n",
        "            c_idx = word2Ind[tweet[j]]\n",
        "            M[w_idx, c_idx] += 1\n",
        "            M[c_idx, w_idx] += 1\n",
        "    return M, word2Ind\n",
        "\n",
        "M, word2Ind = compute_co_occurrence_matrix(twitter_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11njtWHx15cv"
      },
      "source": [
        "## (c) SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LddeVOq615cv"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Run SVD\n",
        "# Note: This may take several minutes (~20-30 minutes)\n",
        "# ------------------------------\n",
        "la = np.linalg\n",
        "U, s, Vh = la.svd(M, full_matrices=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTBT1XolJQnl",
        "outputId": "cdb9e64d-e6e8-45e5-bc4e-eb78a9453fef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-7.84307351e+01  1.31731350e+01  6.29266715e+00 -8.53945247e+00\n",
            " -1.56811281e+00  3.69061675e+00  5.50158212e+00  2.22654627e+00\n",
            "  6.75253024e+00 -5.68496969e+00 -2.95857447e+00 -5.60707114e+00\n",
            " -5.75058186e+00 -6.04011825e+00 -1.04871500e+01 -1.48266916e+01\n",
            "  1.29361686e+01 -2.64826386e+00 -2.78846964e+00 -1.79650257e+00\n",
            "  5.37266122e+00  5.21876584e+00  5.86186791e+00 -9.84126850e+00\n",
            " -1.45755820e+00 -1.72920131e+00 -1.27543562e+00 -3.23865355e+00\n",
            " -1.25375963e+00  1.50121588e+00 -3.89140772e+00  1.20714686e+00\n",
            "  5.74143474e+00  8.43531726e+00  1.49201624e+00  2.93110925e-01\n",
            "  2.10806036e+00 -5.03668509e-01 -5.38797187e+00  3.77701690e-01\n",
            " -1.52282753e+00  3.87760881e+00  2.95026217e+00 -4.12544915e+00\n",
            " -9.45784039e-01  5.70358010e-01 -5.58276097e+00  4.64388492e+00\n",
            "  4.10835612e-01  2.29242881e-02 -8.07158667e-01  1.08967413e+00\n",
            " -4.15866287e+00  6.79476820e+00  1.01967674e+00 -4.59859189e+00\n",
            " -1.80688555e+00  3.27596948e-01  2.46240598e+00 -3.30243894e+00\n",
            " -1.45924423e-01 -8.51104086e-01  3.56893307e+00  1.04026804e+01\n",
            " -5.78550892e+00 -3.48380740e+00  1.17469040e+00  4.33347049e+00\n",
            "  1.26313146e+00  2.95658786e+00 -4.98803799e+00 -7.49014274e-01\n",
            " -4.95401017e+00 -6.67360920e-02 -4.64567288e+00]\n"
          ]
        }
      ],
      "source": [
        "# Compute SVD embeddings\n",
        "embedding_size = 75\n",
        "# S = np.diag(np.sqrt(s[:embedding_size]))\n",
        "# SVD_embeddings = np.dot(U[:, :embedding_size], S)\n",
        "SVD_embeddings = np.dot(U[:,:embedding_size], np.diag(s[:embedding_size]))\n",
        "print(SVD_embeddings[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4dOMjKj15cy"
      },
      "source": [
        "## (d1) Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRti6Rn815cy",
        "outputId": "7c64c138-db4d-43aa-faf2-f9ebc085c190"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Word2Vec model....\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-38-325e5c65de47>:19: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
            "  model.init_sims(replace=True)\n",
            "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
          ]
        }
      ],
      "source": [
        "# Creating the word2vec model and setting values for the various parameters\n",
        "\n",
        "# Initializing the train model. \n",
        "num_features = 75 # Word vector dimensionality\n",
        "min_word_count = 0  # Minimum word count. You can change it also.\n",
        "num_workers = 4     # Number of parallel threads, can be changed\n",
        "context = 5         # Context window size\n",
        "downsampling = 1e-3 # (0.001) Downsample setting for frequent words, can be changed\n",
        "# Initializing the train model\n",
        "print(\"Training Word2Vec model....\")\n",
        "model = word2vec.Word2Vec(twitter_corpus,\n",
        "                          workers=num_workers,\n",
        "                          vector_size=num_features, # API Change to vector_size\n",
        "                          min_count=min_word_count,\n",
        "                          window=context,\n",
        "                          sample=downsampling)\n",
        "\n",
        "# To make the model memory efficient\n",
        "model.init_sims(replace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asqnIK1315c0"
      },
      "source": [
        "## (d2) Compare SVD word embeddings with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwGC7K0z15c3",
        "outputId": "3a4853ec-8e8d-4de7-d786-d2e04278b012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "most similar: [('outbreak', 0.9045967595232447), ('pandemic', 0.902674858151078), ('new', 0.9006981057580782), ('check', 0.8917558169488365), ('fear', 0.8896410163955274), ('due', 0.888897034145722), ('toiletpaper', 0.8861295445542877), ('change', 0.8827712372574316), ('probably', 0.8796619972326563), ('news', 0.8783238923862796)]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def svd_most_similar(query_word, n=10):\n",
        "    \"\"\" return 'n' most similar words of a query word using the SVD word embeddings similar to word2vec's most_smilar    \n",
        "        Params:\n",
        "            query_word (strings): a query word\n",
        "        Return:\n",
        "            most_similar (list of strings): the list of 'n' most similar words\n",
        "    \"\"\"\n",
        "    # get index of a query_word\n",
        "    query_word_idx = word2Ind[query_word]\n",
        "    # get word embedding for a query_word\n",
        "    word = SVD_embeddings[query_word_idx]\n",
        "    #cosine similarity matrix\n",
        "    cos_similarity = cosine_similarity(SVD_embeddings, word.reshape(1, -1))\n",
        "    most_similar = []\n",
        "    # model.wv.most_similar(query_word)\n",
        "    '''\n",
        "        Write additional code to compute the list most_similar. Each entry in the list is a tuple (w, cos)\n",
        "        where w is one of the most similar word to query_word and cos is cosine similarity of w with query_word\n",
        "    '''\n",
        "    # get index of top n most similar words\n",
        "    similar_i = np.argsort(-cos_similarity.flatten())[1:n+1]\n",
        "    \n",
        "    # get similar words and cos_sim score\n",
        "    for i in similar_i:\n",
        "      word = list(word2Ind.keys())[i]\n",
        "      cos_sim = cos_similarity[i][0]\n",
        "      most_similar.append((word, cos_sim))\n",
        "\n",
        "    # sort decreasing based on second item in tuple\n",
        "    most_similar.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return most_similar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Fbmd6y15c4"
      },
      "source": [
        "## SVD vs Word2Vec: \"???\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBzFmNpH15c5",
        "outputId": "e6091030-4b94-446b-aca7-23e7b6cc9029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('outbreak', 0.9045967595232447),\n",
              " ('pandemic', 0.902674858151078),\n",
              " ('new', 0.9006981057580782),\n",
              " ('check', 0.8917558169488365),\n",
              " ('fear', 0.8896410163955274),\n",
              " ('due', 0.888897034145722),\n",
              " ('toiletpaper', 0.8861295445542877),\n",
              " ('change', 0.8827712372574316),\n",
              " ('probably', 0.8796619972326563),\n",
              " ('news', 0.8783238923862796)]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svd_most_similar(\"covid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxVYovyL15c5",
        "outputId": "4379b587-5e01-44db-95b5-2959db3bef74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('panicbuying', 0.9997747540473938),\n",
              " ('coronaoutbreak', 0.9997419714927673),\n",
              " ('coronavirus', 0.9997290372848511),\n",
              " ('pandemic', 0.9997248649597168),\n",
              " ('coronavirusoutbreak', 0.9996583461761475),\n",
              " ('lockdown', 0.9996491074562073),\n",
              " ('corona', 0.9996290802955627),\n",
              " ('coronapocalypse', 0.9996092319488525),\n",
              " ('uk', 0.9996013045310974),\n",
              " ('due', 0.9995934963226318)]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(\"covid\") #this word2vec trained model on tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWWyBDGPpYcp",
        "outputId": "485ab171-cf41-48ee-fef7-8bf6b80dddf6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('mailing', 0.8919418102338428),\n",
              " ('mall', 0.8866803774039633),\n",
              " ('liquor', 0.8831283831185118),\n",
              " ('ht', 0.8790862027429133),\n",
              " ('accusations', 0.8787353350030835),\n",
              " ('elys', 0.8784858187576227),\n",
              " ('llama', 0.8754550627530243),\n",
              " ('dollargeneral', 0.875004724156989),\n",
              " ('pajama', 0.8742027409938027),\n",
              " ('quarterly', 0.8737884191106725)]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svd_most_similar(\"grocery\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcsipeIhpaHh",
        "outputId": "5e160d3a-3e4d-4c83-de8e-d918a39bf173"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('went', 0.9979035258293152),\n",
              " ('shelves', 0.9971742033958435),\n",
              " ('empty', 0.9971593618392944),\n",
              " ('local', 0.9970316290855408),\n",
              " ('today', 0.9967623353004456),\n",
              " ('no', 0.9967173933982849),\n",
              " ('retail', 0.9967158436775208),\n",
              " ('bread', 0.9966362714767456),\n",
              " ('packs', 0.9966211318969727),\n",
              " ('pasta', 0.9966022968292236)]"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.wv.most_similar(\"grocery\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1X7P3q-pcEK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
