{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QjHK80jR15cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Han\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Han\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32md:\\Disk D Documents\\Y2S2\\CS420 AS2\\Q4_template_tweets.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Disk%20D%20Documents/Y2S2/CS420%20AS2/Q4_template_tweets.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Disk%20D%20Documents/Y2S2/CS420%20AS2/Q4_template_tweets.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Disk%20D%20Documents/Y2S2/CS420%20AS2/Q4_template_tweets.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m word2vec\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Disk%20D%20Documents/Y2S2/CS420%20AS2/Q4_template_tweets.ipynb#W0sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Disk%20D%20Documents/Y2S2/CS420%20AS2/Q4_template_tweets.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/drive\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gensim.models import word2vec\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import re # For regular expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTam_xXUv2Z9"
      },
      "source": [
        "## (a) Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQsTJcH2X-9E"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\" Read tweets from the file.\n",
        "        Return:\n",
        "            list of lists (list_words), with words from each of the processed tweets\n",
        "    \"\"\"\n",
        "    tweets = pd.read_csv('', names=['text'])\n",
        "    list_words = []\n",
        "    ### iterate over all tweets from the dataset\n",
        "    for i in tweets.index:\n",
        "      ### remove non-letter.\n",
        "      text = \n",
        "      ### tokenize\n",
        "      words = \n",
        "      \n",
        "      new_words = []\n",
        "      ### iterate over all words of a tweet\n",
        "      for w in words:\n",
        "        ## TODO: remove the stop words and convert a word (w) to the lower case\n",
        "        \n",
        "      list_words.append(new_words)\n",
        "    return list_words\n",
        "\n",
        "# check a few samples of twitter corpus\n",
        "twitter_corpus = load_data()\n",
        "print(twitter_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-ZkbmSX15ck"
      },
      "source": [
        "## (b) Create co-occurrence matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3B83uir15cn"
      },
      "outputs": [],
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" get a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "        Return:\n",
        "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "    # ------------------\n",
        "    # TODO:\n",
        "    # ------------------\n",
        "    return corpus_words, num_corpus_words\n",
        "\n",
        "words, num_words = distinct_words(twitter_corpus)\n",
        "print(words[:10], num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WB4cZBR15cp",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=5):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 5).    \n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (numpy matrix of shape = [number of corpus words x number of corpus words]): \n",
        "                Co-occurence matrix of word counts. \n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    M = None\n",
        "    word2Ind = {}\n",
        "    \n",
        "    # ------------------\n",
        "    # TODO:\n",
        "    # ------------------\n",
        "\n",
        "    return M, word2Ind\n",
        "\n",
        "M, word2Ind = compute_co_occurrence_matrix(twitter_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11njtWHx15cv"
      },
      "source": [
        "## (c) SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LddeVOq615cv"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Run SVD\n",
        "# Note: This may take several minutes (~20-30 minutes)\n",
        "# ------------------------------\n",
        "U, s, Vh = ?\n",
        "SVD_embeddings = ?\n",
        "print(SVD_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4dOMjKj15cy"
      },
      "source": [
        "## (d1) Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRti6Rn815cy"
      },
      "outputs": [],
      "source": [
        "# Creating the word2vec model and setting values for the various parameters\n",
        "\n",
        "# Initializing the train model. \n",
        "num_features = ??   # Word vector dimensionality\n",
        "min_word_count = 0  # Minimum word count. You can change it also.\n",
        "num_workers = 4     # Number of parallel threads, can be changed\n",
        "context = ??         # Context window size\n",
        "downsampling = 1e-3 # (0.001) Downsample setting for frequent words, can be changed\n",
        "# Initializing the train model\n",
        "print(\"Training Word2Vec model....\")\n",
        "model = word2vec.Word2Vec(??)\n",
        "\n",
        "# To make the model memory efficient\n",
        "model.init_sims(replace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asqnIK1315c0"
      },
      "source": [
        "## (d2) Compare SVD word embeddings with Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwGC7K0z15c3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def svd_most_similar(query_word, n=10):\n",
        "    \"\"\" return 'n' most similar words of a query word using the SVD word embeddings similar to word2vec's most_smilar    \n",
        "        Params:\n",
        "            query_word (strings): a query word\n",
        "        Return:\n",
        "            most_similar (list of strings): the list of 'n' most similar words\n",
        "    \"\"\"\n",
        "    # get index of a query_word\n",
        "    query_word_idx = word2Ind[query_word]\n",
        "    # get word embedding for a query_word\n",
        "    word = SVD_embeddings[query_word_idx]\n",
        "    #cosine similarity matrix\n",
        "    cos_similarity = cosine_similarity(SVD_embeddings, word.reshape(1, -1))\n",
        "    most_similar = []\n",
        "    'Write additional code to compute the list most_similar. Each entry in the list is a tuple (w, cos)\n",
        "    'where w is one of the most similar word to query_word and cos is cosine similarity of w with query_word\n",
        "\n",
        "    return most_similar   \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Fbmd6y15c4"
      },
      "source": [
        "## SVD vs Word2Vec: \"???\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBzFmNpH15c5"
      },
      "outputs": [],
      "source": [
        "svd_most_similar(\"covid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxVYovyL15c5"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(\"covid\") #this word2vec trained model on tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWWyBDGPpYcp"
      },
      "outputs": [],
      "source": [
        "svd_most_similar(\"grocery\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcsipeIhpaHh"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(\"grocery\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1X7P3q-pcEK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
