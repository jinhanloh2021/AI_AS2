{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"QjHK80jR15cb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680022524351,"user_tz":-480,"elapsed":7398,"user":{"displayName":"LOH JIN HAN _","userId":"14752307260058667214"}},"outputId":"316b8301-1066-436b-ca6d-cde8cccff0fb"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","\n","import numpy as np\n","import pandas as pd\n","from gensim.models import word2vec\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import re # For regular expressions"]},{"cell_type":"markdown","metadata":{"id":"UTam_xXUv2Z9"},"source":["## (a) Load the dataset"]},{"cell_type":"code","source":["import re\n","def load_data():\n","    \"\"\" Read tweets from the file.\n","        Return:\n","            list of lists (list_words), with words from each of the processed tweets\n","    \"\"\"\n","    tweets = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/AI_AS2/Corona_Tweets.csv', names=['text'])\n","    list_words = []\n","    ### iterate over all tweets from the dataset\n","    for i in tweets.index:\n","      ### remove URLs\n","      text = re.sub(\"https?://\\S+|www\\.\\S+\", \" \", tweets.loc[i, 'text'])\n","      ### remove non-letter.\n","      text = re.sub(\"[^a-zA-Z]\",\" \",text)\n","      ### tokenize\n","      words = text.split()\n","      \n","      new_words = []\n","      ### iterate over all words of a tweet\n","      for w in words:\n","        ## TODO: remove the stop words and convert a word (w) to the lower case\n","        stops = set(stopwords.words(\"english\"))\n","        if w not in stops:\n","          new_words.append(w.lower())\n","        \n","      list_words.append(new_words)\n","    return list_words\n","\n","# check a few samples of twitter corpus\n","twitter_corpus = load_data()\n","print(twitter_corpus[:3])"],"metadata":{"id":"xQsTJcH2X-9E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680022545198,"user_tz":-480,"elapsed":19116,"user":{"displayName":"LOH JIN HAN _","userId":"14752307260058667214"}},"outputId":"8bc1d031-ccc9-4a68-84dc-d648d775dd1a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["[['trending', 'new', 'yorkers', 'encounter', 'empty', 'supermarket', 'shelves', 'pictured', 'wegmans', 'brooklyn', 'sold', 'online', 'grocers', 'foodkick', 'maxdelivery', 'coronavirus', 'fearing', 'shoppers', 'stock'], ['when', 'i', 'find', 'hand', 'sanitizer', 'fred', 'meyer', 'i', 'turned', 'amazon', 'but', 'pack', 'purell', 'check', 'coronavirus', 'concerns', 'driving', 'prices'], ['find', 'protect', 'loved', 'ones', 'coronavirus']]\n"]}]},{"cell_type":"markdown","metadata":{"id":"e-ZkbmSX15ck"},"source":["## (b) Create co-occurrence matrix"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"X3B83uir15cn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680022557026,"user_tz":-480,"elapsed":323,"user":{"displayName":"LOH JIN HAN _","userId":"14752307260058667214"}},"outputId":"5af8d1f1-c468-4a78-8988-032c5ab7a43d"},"outputs":[{"output_type":"stream","name":"stdout","text":["['a', 'aadya', 'aadyasitara', 'aamiin', 'aapl', 'abajam', 'abandon', 'abandoning', 'abc', 'abeg'] 11454\n"]}],"source":["def distinct_words(corpus):\n","    \"\"\" get a list of distinct words for the corpus.\n","        Params:\n","            corpus (list of list of strings): corpus of documents\n","        Return:\n","            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n","            num_corpus_words (integer): number of distinct words across the corpus\n","    \"\"\"\n","    corpus_words = set()\n","    for tweet in corpus:\n","      for word in tweet:\n","        corpus_words.add(word)\n","    corpus_words = sorted(list(corpus_words))\n","    num_corpus_words = len(corpus_words)\n","    return corpus_words, num_corpus_words\n","\n","words, num_words = distinct_words(twitter_corpus)\n","print(words[:10], num_words)"]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true,"id":"8WB4cZBR15cp","executionInfo":{"status":"ok","timestamp":1680022562222,"user_tz":-480,"elapsed":2961,"user":{"displayName":"LOH JIN HAN _","userId":"14752307260058667214"}}},"outputs":[],"source":["def compute_co_occurrence_matrix(corpus, window_size=5):\n","    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 5).    \n","        Params:\n","            corpus (list of list of strings): corpus of documents\n","            window_size (int): size of context window\n","        Return:\n","            M (numpy matrix of shape = [number of corpus words x number of corpus words]): \n","                Co-occurence matrix of word counts. \n","                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n","            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n","    \"\"\"\n","    M = np.zeros((num_words, num_words), dtype=int)\n","    word2Ind = {}\n","    for i, w in enumerate(words):\n","      word2Ind[w] = i\n","    for tweet in corpus:\n","      for i, w in enumerate(tweet):\n","        w_idx = word2Ind[w]\n","        start = i - 5\n","        end = i + 5 + 1 #exclusive\n","        for j in range(start, end):\n","          if(i != j and j >= 0 and j < len(tweet)):\n","            c_idx = word2Ind[tweet[j]]\n","            M[w_idx, c_idx] += 1\n","            M[c_idx, w_idx] += 1\n","    return M, word2Ind\n","\n","M, word2Ind = compute_co_occurrence_matrix(twitter_corpus)"]},{"cell_type":"markdown","metadata":{"id":"11njtWHx15cv"},"source":["## (c) SVD"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"LddeVOq615cv","executionInfo":{"status":"ok","timestamp":1680024039784,"user_tz":-480,"elapsed":1475309,"user":{"displayName":"LOH JIN HAN _","userId":"14752307260058667214"}}},"outputs":[],"source":["# -----------------------------\n","# Run SVD\n","# Note: This may take several minutes (~20-30 minutes)\n","# ------------------------------\n","la = np.linalg\n","U, s, Vh = la.svd(M, full_matrices=False)"]},{"cell_type":"code","source":["# Compute SVD embeddings\n","embedding_size = 75\n","S = np.diag(np.sqrt(s[:embedding_size]))\n","SVD_embeddings = np.dot(U[:, :embedding_size], S)\n","\n","print(SVD_embeddings)\n","print(len(SVD_embeddings))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LTBT1XolJQnl","executionInfo":{"status":"ok","timestamp":1680024278022,"user_tz":-480,"elapsed":7,"user":{"displayName":"LOH JIN HAN _","userId":"14752307260058667214"}},"outputId":"72ff1dac-7435-4df9-dee1-1b8b66a24aac"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-1.14155327e+00  3.37115115e-01  1.68194484e-01 ... -5.13463566e-01\n","  -6.91859966e-03 -4.83082296e-01]\n"," [-1.78800131e-02 -2.31200903e-02 -1.16629915e-03 ...  9.58262009e-03\n","   7.46734194e-04 -1.39529727e-03]\n"," [-2.73409975e-03 -2.25576184e-03  3.02538431e-04 ...  1.03052699e-02\n","  -1.37167111e-03 -3.58876377e-03]\n"," ...\n"," [-1.14900967e-02 -6.29385799e-03  1.19180505e-03 ...  6.77193704e-03\n","   2.41658171e-02 -7.05097585e-03]\n"," [-8.40656902e-03 -4.38477606e-03  2.97293796e-04 ...  3.96570904e-03\n","  -6.79385620e-04  2.98318718e-03]\n"," [-3.37930568e-04 -5.01689942e-04 -1.51688773e-04 ...  2.74498963e-03\n","   1.18551928e-02 -4.63090044e-03]]\n","11454\n"]}]},{"cell_type":"markdown","metadata":{"id":"D4dOMjKj15cy"},"source":["## (d1) Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRti6Rn815cy"},"outputs":[],"source":["# Creating the word2vec model and setting values for the various parameters\n","\n","# Initializing the train model. \n","num_features = ??   # Word vector dimensionality\n","min_word_count = 0  # Minimum word count. You can change it also.\n","num_workers = 4     # Number of parallel threads, can be changed\n","context = ??         # Context window size\n","downsampling = 1e-3 # (0.001) Downsample setting for frequent words, can be changed\n","# Initializing the train model\n","print(\"Training Word2Vec model....\")\n","model = word2vec.Word2Vec(??)\n","\n","# To make the model memory efficient\n","model.init_sims(replace=True)"]},{"cell_type":"markdown","metadata":{"id":"asqnIK1315c0"},"source":["## (d2) Compare SVD word embeddings with Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwGC7K0z15c3"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","\n","def svd_most_similar(query_word, n=10):\n","    \"\"\" return 'n' most similar words of a query word using the SVD word embeddings similar to word2vec's most_smilar    \n","        Params:\n","            query_word (strings): a query word\n","        Return:\n","            most_similar (list of strings): the list of 'n' most similar words\n","    \"\"\"\n","    # get index of a query_word\n","    query_word_idx = word2Ind[query_word]\n","    # get word embedding for a query_word\n","    word = SVD_embeddings[query_word_idx]\n","    #cosine similarity matrix\n","    cos_similarity = cosine_similarity(SVD_embeddings, word.reshape(1, -1))\n","    most_similar = []\n","    'Write additional code to compute the list most_similar. Each entry in the list is a tuple (w, cos)\n","    'where w is one of the most similar word to query_word and cos is cosine similarity of w with query_word\n","\n","    return most_similar   \n","    "]},{"cell_type":"markdown","metadata":{"id":"W3Fbmd6y15c4"},"source":["## SVD vs Word2Vec: \"???\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBzFmNpH15c5"},"outputs":[],"source":["svd_most_similar(\"covid\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BxVYovyL15c5"},"outputs":[],"source":["model.wv.most_similar(\"covid\") #this word2vec trained model on tweets"]},{"cell_type":"code","source":["svd_most_similar(\"grocery\")"],"metadata":{"id":"tWWyBDGPpYcp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.wv.most_similar(\"grocery\")"],"metadata":{"id":"UcsipeIhpaHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"A1X7P3q-pcEK"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}